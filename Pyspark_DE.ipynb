{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pyspark DE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN+FiMTCfBz01+HwKKs+m1r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WanjohiChristopher/Apache-Pyspark/blob/main/Pyspark_DE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTlu_qaIqYsO",
        "outputId": "1f823c1e-d2f1-4d4b-c884-24c20a1fd321"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm9MQOdipyhG",
        "outputId": "3bbaa988-2d3d-4a8d-bbf8-7448e0e670d4"
      },
      "source": [
        "# install py4j\n",
        "!pip install py4j\n",
        "\n",
        "# innstall java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.2/spark-3.0.2-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.2-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark\n",
        "\n",
        "# Install pyspark\n",
        "!pip install pyspark"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: py4j in /usr/local/lib/python3.7/dist-packages (0.10.9.2)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIIi8Q9Xqz-2"
      },
      "source": [
        "# findspark will locate spark in the system\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZHPEVuYq1Jv",
        "outputId": "9b24a19c-997f-41c0-942c-6868f14254cb"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate\n",
        "spark"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method SparkSession.Builder.getOrCreate of <pyspark.sql.session.SparkSession.Builder object at 0x7ff609e04410>>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX_wUUryq65W"
      },
      "source": [
        "# Import dependencies \n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "\n",
        "# StringIndexer is similar to labelencoder which gives a label to each category\n",
        "# OneHotEncoder created onehot encoding vector\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "\n",
        "# VectorAssembler is used to create vector from the features. MOdeling takes vector as an input\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# DecisionTreeClassifier is used for classification problems\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "# For regression problems\n",
        "from pyspark.ml.classification import LogisticRegression"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCS265tkrIJG"
      },
      "source": [
        "#reading data\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Classify Urls\").getOrCreate()\n",
        "df =spark.read.csv('/content/drive/MyDrive/Weatherdata/weatherHistory.csv', \n",
        "                         header=True, \n",
        "                         multiLine=True, \n",
        "                         ignoreLeadingWhiteSpace=True, \n",
        "                         ignoreTrailingWhiteSpace=True, \n",
        "                         encoding=\"UTF-8\",\n",
        "                         sep=',',\n",
        "                         quote='\"', \n",
        "                         escape='\"'\n",
        "                        )"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZEG30glrpMs",
        "outputId": "2b8f4b19-01dd-47a4-cc4b-6239c96055b0"
      },
      "source": [
        "df.show(10)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------+-----------+-----------------+------------------------+--------+------------------+----------------------+------------------+----------+--------------------+--------------------+\n",
            "|      Formatted Date|      Summary|Precip Type|  Temperature (C)|Apparent Temperature (C)|Humidity| Wind Speed (km/h)|Wind Bearing (degrees)|   Visibility (km)|Loud Cover|Pressure (millibars)|       Daily Summary|\n",
            "+--------------------+-------------+-----------+-----------------+------------------------+--------+------------------+----------------------+------------------+----------+--------------------+--------------------+\n",
            "|2006-04-01 00:00:...|Partly Cloudy|       rain|9.472222222222221|      7.3888888888888875|    0.89|           14.1197|                 251.0|15.826300000000002|       0.0|             1015.13|Partly cloudy thr...|\n",
            "|2006-04-01 01:00:...|Partly Cloudy|       rain|9.355555555555558|       7.227777777777776|    0.86|           14.2646|                 259.0|15.826300000000002|       0.0|             1015.63|Partly cloudy thr...|\n",
            "|2006-04-01 02:00:...|Mostly Cloudy|       rain|9.377777777777778|       9.377777777777778|    0.89|3.9284000000000003|                 204.0|           14.9569|       0.0|             1015.94|Partly cloudy thr...|\n",
            "|2006-04-01 03:00:...|Partly Cloudy|       rain| 8.28888888888889|       5.944444444444446|    0.83|           14.1036|                 269.0|15.826300000000002|       0.0|             1016.41|Partly cloudy thr...|\n",
            "|2006-04-01 04:00:...|Mostly Cloudy|       rain|8.755555555555553|       6.977777777777779|    0.83|           11.0446|                 259.0|15.826300000000002|       0.0|             1016.51|Partly cloudy thr...|\n",
            "|2006-04-01 05:00:...|Partly Cloudy|       rain|9.222222222222221|        7.11111111111111|    0.85|           13.9587|                 258.0|           14.9569|       0.0|             1016.66|Partly cloudy thr...|\n",
            "|2006-04-01 06:00:...|Partly Cloudy|       rain|7.733333333333334|       5.522222222222221|    0.95|           12.3648|                 259.0| 9.982000000000001|       0.0|             1016.72|Partly cloudy thr...|\n",
            "|2006-04-01 07:00:...|Partly Cloudy|       rain| 8.77222222222222|       6.527777777777778|    0.89|           14.1519|                 260.0| 9.982000000000001|       0.0|             1016.84|Partly cloudy thr...|\n",
            "|2006-04-01 08:00:...|Partly Cloudy|       rain|10.82222222222222|       10.82222222222222|    0.82|           11.3183|                 259.0| 9.982000000000001|       0.0|             1017.37|Partly cloudy thr...|\n",
            "|2006-04-01 09:00:...|Partly Cloudy|       rain|13.77222222222222|       13.77222222222222|    0.72|12.525800000000002|                 279.0| 9.982000000000001|       0.0|             1017.22|Partly cloudy thr...|\n",
            "+--------------------+-------------+-----------+-----------------+------------------------+--------+------------------+----------------------+------------------+----------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HJrPMSRwSRH",
        "outputId": "9eb68b42-0484-42c9-bd68-39397ad01cf8"
      },
      "source": [
        "#Checking the columns\n",
        "df.columns"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Formatted Date',\n",
              " 'Summary',\n",
              " 'Precip Type',\n",
              " 'Temperature (C)',\n",
              " 'Apparent Temperature (C)',\n",
              " 'Humidity',\n",
              " 'Wind Speed (km/h)',\n",
              " 'Wind Bearing (degrees)',\n",
              " 'Visibility (km)',\n",
              " 'Loud Cover',\n",
              " 'Pressure (millibars)',\n",
              " 'Daily Summary']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1c7ywmUxleZ",
        "outputId": "521b08f3-4ca8-4a4a-afdd-8cf1f66747c7"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Formatted Date', 'string'),\n",
              " ('Summary', 'string'),\n",
              " ('Precip Type', 'string'),\n",
              " ('Temperature (C)', 'string'),\n",
              " ('Apparent Temperature (C)', 'string'),\n",
              " ('Humidity', 'string'),\n",
              " ('Wind Speed (km/h)', 'string'),\n",
              " ('Wind Bearing (degrees)', 'string'),\n",
              " ('Visibility (km)', 'string'),\n",
              " ('Loud Cover', 'string'),\n",
              " ('Pressure (millibars)', 'string'),\n",
              " ('Daily Summary', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpErfvDwxo0m",
        "outputId": "147aa3af-0300-442b-b821-9c69fe192e85"
      },
      "source": [
        "# Printing the table schema\n",
        "df.printSchema()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Formatted Date: string (nullable = true)\n",
            " |-- Summary: string (nullable = true)\n",
            " |-- Precip Type: string (nullable = true)\n",
            " |-- Temperature (C): string (nullable = true)\n",
            " |-- Apparent Temperature (C): string (nullable = true)\n",
            " |-- Humidity: string (nullable = true)\n",
            " |-- Wind Speed (km/h): string (nullable = true)\n",
            " |-- Wind Bearing (degrees): string (nullable = true)\n",
            " |-- Visibility (km): string (nullable = true)\n",
            " |-- Loud Cover: string (nullable = true)\n",
            " |-- Pressure (millibars): string (nullable = true)\n",
            " |-- Daily Summary: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eagiHJSdzTRY",
        "outputId": "0f94415e-c5e4-4ab3-ed8c-85c198b4db6c"
      },
      "source": [
        "#counting records\n",
        "df.count()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96453"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W56QQ1OWz1hq",
        "outputId": "8962f8c8-daaa-4f29-d440-a06610a9d0b0"
      },
      "source": [
        "# We perform  summary statistics\n",
        "df.describe().show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------------------+-----------+--------------------+------------------------+-------------------+------------------+----------------------+------------------+----------+--------------------+--------------------+\n",
            "|summary|      Formatted Date|             Summary|Precip Type|     Temperature (C)|Apparent Temperature (C)|           Humidity| Wind Speed (km/h)|Wind Bearing (degrees)|   Visibility (km)|Loud Cover|Pressure (millibars)|       Daily Summary|\n",
            "+-------+--------------------+--------------------+-----------+--------------------+------------------------+-------------------+------------------+----------------------+------------------+----------+--------------------+--------------------+\n",
            "|  count|               96453|               96453|      96453|               96453|                   96453|              96453|             96453|                 96453|             96453|     96453|               96453|               96453|\n",
            "|   mean|                null|                null|       null|  11.932678437511868|      10.855028874166726| 0.7348989663358906|10.810640140793208|    187.50923247592092|10.347324929237148|       0.0|  1003.2359558541606|                null|\n",
            "| stddev|                null|                null|       null|    9.55154632065698|       10.69684739211911|0.19547273906721474| 6.913571012591965|    107.38342838070518| 4.192123191422759|       0.0|   116.9699056825797|                null|\n",
            "|    min|2006-01-01 00:00:...|              Breezy|       null|-0.00555555555555...|    -0.00555555555555...|                0.0|               0.0|                   0.0|               0.0|       0.0|                 0.0|Breezy and foggy ...|\n",
            "|    max|2016-12-31 23:00:...|Windy and Partly ...|       snow|   9.994444444444447|       9.994444444444447|                1.0|            9.9981|                  99.0|            9.9981|       0.0|              999.99|Windy in the afte...|\n",
            "+-------+--------------------+--------------------+-----------+--------------------+------------------------+-------------------+------------------+----------------------+------------------+----------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnDjCKEY0Bxr"
      },
      "source": [
        "# drop rows with missing values\n",
        "df = df.dropna() "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED5GLvpv14a8"
      },
      "source": [
        "\n",
        "x=df.columns[-1]\n",
        "y=df.columns[1] # Dependent Variable\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5drtte7F-lvb",
        "outputId": "bc8b9f2b-9c76-4631-b5e3-5820ff497ded"
      },
      "source": [
        "y"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Summary'"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "8lnWaTPL-J8M",
        "outputId": "6ce692c5-1232-47dc-fefc-4528077556d7"
      },
      "source": [
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=x,outputCol='Summary')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36m_set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeConverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36mtoListString\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTypeConverters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not convert %s to list of strings\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Could not convert Daily Summary to list of strings",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-069f946bd845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0massembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Summary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop3.2/python/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/feature.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputCols, outputCol, handleInvalid)\u001b[0m\n\u001b[1;32m   4130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandleInvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4131\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4132\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4134\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mkeyword_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop3.2/python/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/feature.py\u001b[0m in \u001b[0;36msetParams\u001b[0;34m(self, inputCols, outputCol, handleInvalid)\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \"\"\"\n\u001b[1;32m   4141\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetInputCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36m_set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeConverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid param value given for param \"%s\". %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid param value given for param \"inputCols\". Could not convert Daily Summary to list of strings"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdi0nwor-1Z4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}